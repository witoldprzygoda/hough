{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hough Transform Analysis - OOP Version\n",
    "\n",
    "This notebook contains the complete Object-Oriented Programming refactored version of the Hough Transform Analysis.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The code follows enterprise design patterns:\n",
    "- **Domain Layer**: Core business entities (Particle, Peak, Track, HoughSquare)\n",
    "- **Strategy Pattern**: Interchangeable easing algorithms\n",
    "- **Repository Pattern**: Data access abstraction\n",
    "- **Service Layer**: Business logic (PeakDetector, TrackSlicer, HoughMatcher, Visualizer)\n",
    "- **Facade Pattern**: Simplified high-level API\n",
    "- **Observer Pattern**: Progress tracking\n",
    "- **Builder Pattern**: Flexible configuration\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. Run all cells sequentially (Cell â†’ Run All)\n",
    "2. Modify configuration in the last cell if needed\n",
    "3. Execute the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Type, Iterator\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# ROOT file handling\n",
    "import uproot\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Domain Models - Particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Particle:\n",
    "    \"\"\"Immutable value object representing a particle.\"\"\"\n",
    "    pdg_id: int\n",
    "    charge: float\n",
    "    pt: float\n",
    "    eta: float\n",
    "    phi: float\n",
    "    pz: float\n",
    "    vz: float\n",
    "\n",
    "    @property\n",
    "    def pz_over_pt(self) -> float:\n",
    "        return self.pz / self.pt if self.pt != 0 else 0.0\n",
    "\n",
    "    @property\n",
    "    def cotangent(self) -> float:\n",
    "        return self.pz_over_pt\n",
    "\n",
    "    def is_charged(self) -> bool:\n",
    "        return self.charge != 0\n",
    "\n",
    "\n",
    "class ParticleChargeRegistry:\n",
    "    \"\"\"Singleton registry for PDG particle ID to charge mappings.\"\"\"\n",
    "    \n",
    "    _instance: Optional['ParticleChargeRegistry'] = None\n",
    "    \n",
    "    _CHARGE_MAP: Dict[int, float] = {\n",
    "        # Gauge bosons\n",
    "        22: 0, 23: 0, 24: 1, -24: -1, 21: 0,\n",
    "        # Leptons\n",
    "        11: -1, -11: 1, 12: 0, -12: 0,\n",
    "        13: -1, -13: 1, 14: 0, -14: 0,\n",
    "        15: -1, -15: 1, 16: 0, -16: 0,\n",
    "        # Quarks\n",
    "        1: -1/3, -1: 1/3, 2: 2/3, -2: -2/3,\n",
    "        3: -1/3, -3: 1/3, 4: 2/3, -4: -2/3,\n",
    "        5: -1/3, -5: 1/3, 6: 2/3, -6: -2/3,\n",
    "        # Light mesons\n",
    "        111: 0, 211: 1, -211: -1, 113: 0, 213: 1, -213: -1,\n",
    "        221: 0, 331: 0, 130: 0, 310: 0, 311: 0, -311: 0,\n",
    "        321: 1, -321: -1,\n",
    "        # Charmed mesons\n",
    "        411: 1, -411: -1, 421: 0, -421: 0,\n",
    "        # Bottom mesons\n",
    "        511: 0, -511: 0, 521: 1, -521: -1,\n",
    "        # Baryons\n",
    "        2212: 1, -2212: -1, 2112: 0, -2112: 0,\n",
    "        3122: 0, -3122: 0, 3222: 1, -3222: -1,\n",
    "        3212: 0, -3212: 0, 3112: -1, -3112: 1,\n",
    "        3312: -1, -3312: 1, 3322: 0, -3322: 0,\n",
    "    }\n",
    "\n",
    "    def __new__(cls) -> 'ParticleChargeRegistry':\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def get_charge(self, pdg_id: int) -> Optional[float]:\n",
    "        if pdg_id in self._CHARGE_MAP:\n",
    "            return self._CHARGE_MAP[pdg_id]\n",
    "        if pdg_id < 0 and -pdg_id in self._CHARGE_MAP:\n",
    "            return -self._CHARGE_MAP[-pdg_id]\n",
    "        return None\n",
    "\n",
    "    def get_charge_safe(self, pdg_id: int, default: float = 0.0) -> float:\n",
    "        charge = self.get_charge(pdg_id)\n",
    "        return charge if charge is not None else default\n",
    "\n",
    "print(\"âœ“ Particle models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Domain Models - Peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Peak:\n",
    "    \"\"\"Immutable value object representing a peak in Hough space.\"\"\"\n",
    "    x: float\n",
    "    y: float\n",
    "    height: float\n",
    "\n",
    "    def distance_to(self, other: 'Peak') -> float:\n",
    "        return np.sqrt((self.x - other.x)**2 + (self.y - other.y)**2)\n",
    "\n",
    "    def is_within_tolerance(self, other: 'Peak', tolerance: float) -> bool:\n",
    "        return self.distance_to(other) <= tolerance\n",
    "\n",
    "    def to_tuple(self) -> Tuple[float, float, float]:\n",
    "        return (self.x, self.y, self.height)\n",
    "\n",
    "\n",
    "class PeakCollection:\n",
    "    \"\"\"Collection of peaks with operations.\"\"\"\n",
    "\n",
    "    def __init__(self, peaks: List[Peak] = None):\n",
    "        self._peaks: List[Peak] = peaks if peaks is not None else []\n",
    "\n",
    "    def add(self, peak: Peak) -> None:\n",
    "        self._peaks.append(peak)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._peaks)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._peaks)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Peak:\n",
    "        return self._peaks[index]\n",
    "\n",
    "    def filter_by_height(self, min_height: float) -> 'PeakCollection':\n",
    "        filtered = [p for p in self._peaks if p.height >= min_height]\n",
    "        return PeakCollection(filtered)\n",
    "\n",
    "    def find_within_tolerance(self, target: Peak, tolerance: float) -> List[Peak]:\n",
    "        return [p for p in self._peaks if target.is_within_tolerance(p, tolerance)]\n",
    "\n",
    "    def to_array(self) -> np.ndarray:\n",
    "        if not self._peaks:\n",
    "            return np.empty((0, 3))\n",
    "        return np.array([p.to_tuple() for p in self._peaks])\n",
    "\n",
    "    @classmethod\n",
    "    def from_tuples(cls, tuples: List[Tuple[float, float, float]]) -> 'PeakCollection':\n",
    "        peaks = [Peak(x=t[0], y=t[1], height=t[2]) for t in tuples]\n",
    "        return cls(peaks)\n",
    "\n",
    "print(\"âœ“ Peak models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Domain Models - Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Track:\n",
    "    \"\"\"Represents a track in the detector.\"\"\"\n",
    "    event_id: int\n",
    "    phi_bin: float\n",
    "    curv_bin: float\n",
    "    eta: float\n",
    "    vz: float\n",
    "    number_of_hits: int\n",
    "    pz_over_pt: float\n",
    "    particle_type: int\n",
    "    phi: float\n",
    "    pt: float\n",
    "    pz: float\n",
    "    is_reconstructed: bool = False\n",
    "\n",
    "    @property\n",
    "    def cotangent(self) -> float:\n",
    "        return self.pz_over_pt\n",
    "\n",
    "    def is_in_vz_range(self, vz_min: float, vz_max: float) -> bool:\n",
    "        return vz_min < self.vz < vz_max\n",
    "\n",
    "    def is_in_cot_range(self, cot_min: float, cot_max: float) -> bool:\n",
    "        return cot_min < self.cotangent < cot_max\n",
    "\n",
    "    def mark_reconstructed(self) -> None:\n",
    "        self.is_reconstructed = True\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'event_id': self.event_id,\n",
    "            'phi_bin': self.phi_bin,\n",
    "            'curv_bin': self.curv_bin,\n",
    "            'eta': self.eta,\n",
    "            'vz': self.vz,\n",
    "            'number_of_hits': self.number_of_hits,\n",
    "            'pz_over_pt': self.pz_over_pt,\n",
    "            'particle_type': self.particle_type,\n",
    "            'phi': self.phi,\n",
    "            'pt': self.pt,\n",
    "            'pz': self.pz,\n",
    "            'reco': 1 if self.is_reconstructed else 0,\n",
    "        }\n",
    "\n",
    "\n",
    "class TrackCollection:\n",
    "    \"\"\"Collection of tracks with query and filtering operations.\"\"\"\n",
    "\n",
    "    def __init__(self, tracks: List[Track] = None):\n",
    "        self._tracks: List[Track] = tracks if tracks is not None else []\n",
    "        self._by_event: dict = {}\n",
    "        self._rebuild_index()\n",
    "\n",
    "    def _rebuild_index(self) -> None:\n",
    "        self._by_event = {}\n",
    "        for track in self._tracks:\n",
    "            if track.event_id not in self._by_event:\n",
    "                self._by_event[track.event_id] = []\n",
    "            self._by_event[track.event_id].append(track)\n",
    "\n",
    "    def add(self, track: Track) -> None:\n",
    "        self._tracks.append(track)\n",
    "        if track.event_id not in self._by_event:\n",
    "            self._by_event[track.event_id] = []\n",
    "        self._by_event[track.event_id].append(track)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._tracks)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._tracks)\n",
    "\n",
    "    def filter_by_vz_range(self, vz_min: float, vz_max: float) -> 'TrackCollection':\n",
    "        filtered = [t for t in self._tracks if t.is_in_vz_range(vz_min, vz_max)]\n",
    "        return TrackCollection(filtered)\n",
    "\n",
    "    def filter_by_cot_range(self, cot_min: float, cot_max: float) -> 'TrackCollection':\n",
    "        filtered = [t for t in self._tracks if t.is_in_cot_range(cot_min, cot_max)]\n",
    "        return TrackCollection(filtered)\n",
    "\n",
    "    def get_reconstructed(self) -> 'TrackCollection':\n",
    "        filtered = [t for t in self._tracks if t.is_reconstructed]\n",
    "        return TrackCollection(filtered)\n",
    "\n",
    "    def count_reconstructed(self) -> int:\n",
    "        return sum(1 for t in self._tracks if t.is_reconstructed)\n",
    "\n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        if not self._tracks:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame([t.to_dict() for t in self._tracks])\n",
    "\n",
    "print(\"âœ“ Track models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Domain Models - Hough Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareClassification(Enum):\n",
    "    \"\"\"Classification of Hough squares.\"\"\"\n",
    "    TRUE_POSITIVE = \"true_positive\"\n",
    "    FALSE_POSITIVE = \"false_positive\"\n",
    "    UNCLASSIFIED = \"unclassified\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HoughSquare:\n",
    "    \"\"\"Immutable value object representing a square region in Hough space.\"\"\"\n",
    "    data: np.ndarray\n",
    "    classification: SquareClassification\n",
    "    center_x: float\n",
    "    center_y: float\n",
    "\n",
    "    @property\n",
    "    def is_true_positive(self) -> bool:\n",
    "        return self.classification == SquareClassification.TRUE_POSITIVE\n",
    "\n",
    "    @property\n",
    "    def is_false_positive(self) -> bool:\n",
    "        return self.classification == SquareClassification.FALSE_POSITIVE\n",
    "\n",
    "\n",
    "class HoughSquareCollection:\n",
    "    \"\"\"Collection of Hough squares for training data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._true_positives: List[HoughSquare] = []\n",
    "        self._false_positives: List[HoughSquare] = []\n",
    "\n",
    "    def add_square(self, square: HoughSquare) -> None:\n",
    "        if square.is_true_positive:\n",
    "            self._true_positives.append(square)\n",
    "        elif square.is_false_positive:\n",
    "            self._false_positives.append(square)\n",
    "\n",
    "    @property\n",
    "    def true_positive_count(self) -> int:\n",
    "        return len(self._true_positives)\n",
    "\n",
    "    @property\n",
    "    def false_positive_count(self) -> int:\n",
    "        return len(self._false_positives)\n",
    "\n",
    "    @property\n",
    "    def total_count(self) -> int:\n",
    "        return self.true_positive_count + self.false_positive_count\n",
    "\n",
    "    def get_training_data(self) -> tuple:\n",
    "        if self.total_count == 0:\n",
    "            return np.empty((0, 0)), np.empty(0)\n",
    "\n",
    "        true_data = np.stack([sq.data for sq in self._true_positives], axis=0)\n",
    "        true_labels = np.ones(len(self._true_positives))\n",
    "\n",
    "        false_data = np.stack([sq.data for sq in self._false_positives], axis=0)\n",
    "        false_labels = np.zeros(len(self._false_positives))\n",
    "\n",
    "        X = np.vstack([true_data, false_data])\n",
    "        y = np.hstack([true_labels, false_labels])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def summary(self) -> dict:\n",
    "        return {\n",
    "            'true_positives': self.true_positive_count,\n",
    "            'false_positives': self.false_positive_count,\n",
    "            'total': self.total_count,\n",
    "            'true_positive_ratio': (\n",
    "                self.true_positive_count / self.total_count\n",
    "                if self.total_count > 0 else 0.0\n",
    "            )\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Hough square models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class HoughConfig:\n",
    "    \"\"\"Configuration for Hough transform parameters.\"\"\"\n",
    "    nbin_phi: int = 7000\n",
    "    nbin_qpt: int = 216\n",
    "    square_size: int = 16\n",
    "    tolerance: float = 6.0\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PeakDetectionConfig:\n",
    "    \"\"\"Configuration for peak detection parameters.\"\"\"\n",
    "    threshold_abs: float = 5.0\n",
    "    threshold_rel: float = 0.0\n",
    "    min_distance: int = 2\n",
    "    smooth_sigma: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class VisualizationConfig:\n",
    "    \"\"\"Configuration for visualization parameters.\"\"\"\n",
    "    start_phi: int = 1000\n",
    "    end_phi: int = 2000\n",
    "    size_true: int = 3\n",
    "    enabled: bool = True\n",
    "    max_plots: int = 200\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for data processing.\"\"\"\n",
    "    slice_list: List[int] = field(default_factory=lambda: [-1])\n",
    "    num_files: int = 8\n",
    "    min_hits: int = 4\n",
    "    vz_range: tuple = (-200, 200)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PathConfig:\n",
    "    \"\"\"Configuration for file paths.\"\"\"\n",
    "    data_path: str = \"/eos/user/t/tbold/EFTracking/HoughML/pg_2mu_pu100_insquare\"\n",
    "    output_dir: str = \".\"\n",
    "\n",
    "    @property\n",
    "    def data_path_obj(self) -> Path:\n",
    "        return Path(self.data_path)\n",
    "\n",
    "    @property\n",
    "    def output_dir_obj(self) -> Path:\n",
    "        return Path(self.output_dir)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AnalysisConfig:\n",
    "    \"\"\"Complete analysis configuration.\"\"\"\n",
    "    hough: HoughConfig\n",
    "    peak_detection: PeakDetectionConfig\n",
    "    visualization: VisualizationConfig\n",
    "    processing: ProcessingConfig\n",
    "    paths: PathConfig\n",
    "    easing_type: str = \"InSquare\"\n",
    "\n",
    "\n",
    "class AnalysisConfigBuilder:\n",
    "    \"\"\"Builder for creating AnalysisConfig instances.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._hough: Optional[HoughConfig] = None\n",
    "        self._peak_detection: Optional[PeakDetectionConfig] = None\n",
    "        self._visualization: Optional[VisualizationConfig] = None\n",
    "        self._processing: Optional[ProcessingConfig] = None\n",
    "        self._paths: Optional[PathConfig] = None\n",
    "        self._easing_type: str = \"InSquare\"\n",
    "\n",
    "    def with_paths(self, data_path: str, output_dir: str = \".\") -> 'AnalysisConfigBuilder':\n",
    "        self._paths = PathConfig(data_path=data_path, output_dir=output_dir)\n",
    "        return self\n",
    "\n",
    "    def with_processing_config(self, slice_list: List[int] = None, num_files: int = 8) -> 'AnalysisConfigBuilder':\n",
    "        if slice_list is None:\n",
    "            slice_list = [-1]\n",
    "        self._processing = ProcessingConfig(slice_list=slice_list, num_files=num_files)\n",
    "        return self\n",
    "\n",
    "    def build(self) -> AnalysisConfig:\n",
    "        hough = self._hough or HoughConfig()\n",
    "        peak_detection = self._peak_detection or PeakDetectionConfig()\n",
    "        visualization = self._visualization or VisualizationConfig()\n",
    "        processing = self._processing or ProcessingConfig()\n",
    "        paths = self._paths or PathConfig()\n",
    "\n",
    "        return AnalysisConfig(\n",
    "            hough=hough,\n",
    "            peak_detection=peak_detection,\n",
    "            visualization=visualization,\n",
    "            processing=processing,\n",
    "            paths=paths,\n",
    "            easing_type=self._easing_type\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_legacy_config(cls) -> AnalysisConfig:\n",
    "        return cls().build()\n",
    "\n",
    "print(\"âœ“ Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Strategy Pattern - Easing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasingStrategy(ABC):\n",
    "    \"\"\"Abstract base class for easing strategies.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def ease(self, x: float) -> float:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearEasing(EasingStrategy):\n",
    "    def ease(self, x: float) -> float:\n",
    "        return x\n",
    "    def name(self) -> str:\n",
    "        return \"Linear\"\n",
    "\n",
    "\n",
    "class InSineEasing(EasingStrategy):\n",
    "    def ease(self, x: float) -> float:\n",
    "        sign = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "        return sign * 32 * (1 - math.cos((x * math.pi) / 64))\n",
    "    def name(self) -> str:\n",
    "        return \"InSine\"\n",
    "\n",
    "\n",
    "class InSquareEasing(EasingStrategy):\n",
    "    def ease(self, x: float) -> float:\n",
    "        sign = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "        return sign * 32 * (x * x / 1024)\n",
    "    def name(self) -> str:\n",
    "        return \"InSquare\"\n",
    "\n",
    "\n",
    "class InCubicEasing(EasingStrategy):\n",
    "    def ease(self, x: float) -> float:\n",
    "        return 32 * (x * x * x / 32768)\n",
    "    def name(self) -> str:\n",
    "        return \"InCubic\"\n",
    "\n",
    "\n",
    "class InCircEasing(EasingStrategy):\n",
    "    def ease(self, x: float) -> float:\n",
    "        sign = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "        return sign * (32 - math.sqrt(1024 - x * x))\n",
    "    def name(self) -> str:\n",
    "        return \"InCirc\"\n",
    "\n",
    "\n",
    "class EasingStrategyFactory:\n",
    "    \"\"\"Factory for creating easing strategy instances.\"\"\"\n",
    "    \n",
    "    _strategies: Dict[str, Type[EasingStrategy]] = {\n",
    "        \"Linear\": LinearEasing,\n",
    "        \"InSine\": InSineEasing,\n",
    "        \"InSquare\": InSquareEasing,\n",
    "        \"InCubic\": InCubicEasing,\n",
    "        \"InCirc\": InCircEasing,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, strategy_name: str) -> EasingStrategy:\n",
    "        strategy_class = cls._strategies.get(strategy_name)\n",
    "        if strategy_class is None:\n",
    "            raise ValueError(f\"Unknown easing strategy: {strategy_name}\")\n",
    "        return strategy_class()\n",
    "\n",
    "print(\"âœ“ Strategy pattern defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Repository - Particle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleRepository:\n",
    "    \"\"\"Repository for accessing particle simulation data from ROOT files.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: Path):\n",
    "        self._data_path = data_path\n",
    "        self._charge_registry = ParticleChargeRegistry()\n",
    "        self._cached_df: pd.DataFrame = None\n",
    "\n",
    "    def load_particles(self, pattern: str = \"particles*.root\") -> pd.DataFrame:\n",
    "        root_files = sorted(self._data_path.glob(pattern))\n",
    "        if not root_files:\n",
    "            raise FileNotFoundError(f\"No files matching '{pattern}' found in {self._data_path}\")\n",
    "\n",
    "        print(f\"Found {len(root_files)} ROOT files in {self._data_path}\\n\")\n",
    "\n",
    "        for file in root_files:\n",
    "            print(f\"ðŸ“ {file.name}\")\n",
    "            tree = uproot.open(file)[\"particles\"]\n",
    "            arrays = tree.arrays(library=\"np\")\n",
    "\n",
    "        df = pd.DataFrame(arrays)\n",
    "        self._cached_df = df\n",
    "        print(\"\\nColumn names:\")\n",
    "        print(list(df.columns))\n",
    "        return df\n",
    "\n",
    "    def get_tracks_for_event(self, event_id: int, df: pd.DataFrame = None,\n",
    "                             nbin_phi: int = 7000, nbin_qpt: int = 216,\n",
    "                             min_hits: int = 4) -> TrackCollection:\n",
    "        if df is None:\n",
    "            df = self._cached_df\n",
    "        if df is None:\n",
    "            raise ValueError(\"No data loaded. Call load_particles() first.\")\n",
    "\n",
    "        tracks = []\n",
    "        for row in df.itertuples(index=False):\n",
    "            if row.event_id != event_id:\n",
    "                continue\n",
    "\n",
    "            charges = self._get_charges(row.particle_type)\n",
    "            mask = (np.array(charges) != 0) & (np.array(row.number_of_hits) > min_hits)\n",
    "\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            phi = row.phi\n",
    "            phi_bin = (phi + np.pi) * nbin_phi / (2.0 * np.pi)\n",
    "            curv = np.array(charges) / row.pt\n",
    "            curv_bin = int(nbin_qpt / 2.0) + curv * int(nbin_qpt / 2.0)\n",
    "\n",
    "            for i in np.where(mask)[0]:\n",
    "                track = Track(\n",
    "                    event_id=event_id,\n",
    "                    phi_bin=float(phi_bin[i]),\n",
    "                    curv_bin=float(curv_bin[i]),\n",
    "                    eta=float(row.eta[i]),\n",
    "                    vz=float(row.vz[i]),\n",
    "                    number_of_hits=int(row.number_of_hits[i]),\n",
    "                    pz_over_pt=float(row.pz[i] / row.pt[i]),\n",
    "                    particle_type=int(row.particle_type[i]),\n",
    "                    phi=float(row.phi[i]),\n",
    "                    pt=float(row.pt[i]),\n",
    "                    pz=float(row.pz[i]),\n",
    "                    is_reconstructed=False\n",
    "                )\n",
    "                tracks.append(track)\n",
    "\n",
    "        return TrackCollection(tracks)\n",
    "\n",
    "    def get_all_events(self, df: pd.DataFrame = None) -> List[int]:\n",
    "        if df is None:\n",
    "            df = self._cached_df\n",
    "        if df is None:\n",
    "            raise ValueError(\"No data loaded. Call load_particles() first.\")\n",
    "        return df[\"event_id\"].unique().tolist()\n",
    "\n",
    "    def create_tracks_dict(self, nbin_phi: int = 7000, nbin_qpt: int = 216,\n",
    "                          min_hits: int = 4) -> dict:\n",
    "        if self._cached_df is None:\n",
    "            raise ValueError(\"No data loaded. Call load_particles() first.\")\n",
    "\n",
    "        tracks_dict = {}\n",
    "        event_ids = self.get_all_events()\n",
    "\n",
    "        for event_id in event_ids:\n",
    "            tracks_dict[event_id] = self.get_tracks_for_event(\n",
    "                event_id, self._cached_df, nbin_phi, nbin_qpt, min_hits\n",
    "            )\n",
    "\n",
    "        return tracks_dict\n",
    "\n",
    "    def _get_charges(self, particle_types) -> List[float]:\n",
    "        charges = []\n",
    "        for pid in particle_types:\n",
    "            charge = self._charge_registry.get_charge_safe(pid)\n",
    "            charges.append(charge)\n",
    "        return charges\n",
    "\n",
    "    def save_tracks_to_root(self, tracks_dict: dict, event_list: List[int],\n",
    "                           filename: str, treename: str = \"ntuple\") -> pd.DataFrame:\n",
    "        combined_dfs = []\n",
    "        for event_id in event_list:\n",
    "            if event_id in tracks_dict:\n",
    "                collection = tracks_dict[event_id]\n",
    "                df = collection.to_dataframe()\n",
    "                if not df.empty:\n",
    "                    combined_dfs.append(df)\n",
    "\n",
    "        if not combined_dfs:\n",
    "            raise ValueError(\"No tracks to save\")\n",
    "\n",
    "        combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "        print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "\n",
    "        with uproot.recreate(filename) as file:\n",
    "            file[treename] = combined_df.to_dict(orient='list')\n",
    "\n",
    "        print(f\"Successfully saved to {filename}\")\n",
    "        return combined_df\n",
    "\n",
    "print(\"âœ“ Particle repository defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Repository - Hough Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoughHistogram:\n",
    "    \"\"\"Value object representing a Hough histogram.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, histogram, event_id: int, slice_num: int):\n",
    "        self.name = name\n",
    "        self.histogram = histogram\n",
    "        self.event_id = event_id\n",
    "        self.slice_num = slice_num\n",
    "\n",
    "\n",
    "class HoughRepository:\n",
    "    \"\"\"Repository for accessing Hough accumulator data from ROOT files.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: Path):\n",
    "        self._data_path = data_path\n",
    "\n",
    "    def find_files(self, pattern: str = \"out*.root\") -> List[Path]:\n",
    "        root_files = sorted(self._data_path.glob(pattern))\n",
    "        print(f\"Found {len(root_files)} ROOT files in {self._data_path}\\n\")\n",
    "        return root_files\n",
    "\n",
    "    def iter_histograms(self, file_path: Path) -> Iterator[HoughHistogram]:\n",
    "        with uproot.open(file_path) as f:\n",
    "            for key in f.keys():\n",
    "                obj = f[key]\n",
    "                if not obj.classname.startswith(\"TH2\"):\n",
    "                    continue\n",
    "\n",
    "                event_id, slice_num = self._parse_histogram_name(key)\n",
    "                yield HoughHistogram(\n",
    "                    name=key,\n",
    "                    histogram=obj,\n",
    "                    event_id=event_id,\n",
    "                    slice_num=slice_num\n",
    "                )\n",
    "\n",
    "    def _parse_histogram_name(self, name: str) -> Tuple[int, int]:\n",
    "        parts = name.replace('_', ';').split(';')\n",
    "        return int(parts[1]), int(parts[2])\n",
    "\n",
    "print(\"âœ“ Hough repository defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Repository - Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataRepository:\n",
    "    \"\"\"Repository for saving and loading training data.\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self._output_dir = output_dir\n",
    "        self._output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save_squares(self, collection: HoughSquareCollection,\n",
    "                     filename: str = \"images.npz\") -> None:\n",
    "        X, y = collection.get_training_data()\n",
    "        file_path = self._output_dir / filename\n",
    "        np.savez(file_path, X=X, y=y)\n",
    "\n",
    "        summary = collection.summary()\n",
    "        print(f\"\\nTraining data saved to {file_path}\")\n",
    "        print(f\"True positives: {summary['true_positives']}\")\n",
    "        print(f\"False positives: {summary['false_positives']}\")\n",
    "        print(f\"Total samples: {summary['total']}\")\n",
    "        print(f\"Array shapes: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "print(\"âœ“ Training data repository defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Service - Peak Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeakDetectorService:\n",
    "    \"\"\"Service for detecting peaks in 2D Hough accumulator.\"\"\"\n",
    "\n",
    "    def __init__(self, config: PeakDetectionConfig):\n",
    "        self._config = config\n",
    "\n",
    "    def find_peaks(self, histogram) -> PeakCollection:\n",
    "        values, xedges, yedges = histogram.to_numpy()\n",
    "        values = values.T\n",
    "        values = np.nan_to_num(values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if self._config.smooth_sigma > 0:\n",
    "            values = gaussian_filter(values, sigma=self._config.smooth_sigma)\n",
    "\n",
    "        coords = self._vectorized_sliding_peaks(\n",
    "            values,\n",
    "            min_height=self._config.threshold_abs,\n",
    "            window_size=2 * self._config.min_distance\n",
    "        )\n",
    "\n",
    "        xcenters = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "        ycenters = 0.5 * (yedges[1:] + yedges[:-1])\n",
    "\n",
    "        peaks = []\n",
    "        for (y_idx, x_idx) in coords:\n",
    "            peak = Peak(\n",
    "                x=float(xcenters[x_idx]),\n",
    "                y=float(ycenters[y_idx]),\n",
    "                height=float(values[y_idx, x_idx])\n",
    "            )\n",
    "            peaks.append(peak)\n",
    "\n",
    "        del xedges, yedges, xcenters, ycenters, coords\n",
    "        gc.collect()\n",
    "\n",
    "        return PeakCollection(peaks)\n",
    "\n",
    "    def _vectorized_sliding_peaks(self, matrix: np.ndarray,\n",
    "                                   min_height: float = 6,\n",
    "                                   window_size: int = 5) -> np.ndarray:\n",
    "        if window_size % 2 == 0:\n",
    "            window_size += 1\n",
    "\n",
    "        half_window = window_size // 2\n",
    "        windows_2d = sliding_window_view(matrix, (window_size, window_size))\n",
    "\n",
    "        center_r, center_c = half_window, half_window\n",
    "        center_values = windows_2d[:, :, center_r, center_c]\n",
    "\n",
    "        is_peak = np.ones_like(center_values, dtype=bool)\n",
    "        window_maxima = np.max(windows_2d, axis=(2, 3))\n",
    "        is_peak &= (center_values == window_maxima)\n",
    "        is_peak &= (center_values >= min_height)\n",
    "\n",
    "        peak_rows, peak_cols = np.where(is_peak)\n",
    "        adjusted_rows = peak_rows + half_window\n",
    "        adjusted_cols = peak_cols + half_window\n",
    "\n",
    "        if len(adjusted_rows) > 0:\n",
    "            peak_coords = np.stack((adjusted_rows, adjusted_cols), axis=1)\n",
    "            peak_values = matrix[adjusted_rows, adjusted_cols]\n",
    "\n",
    "            distances = cdist(peak_coords, peak_coords)\n",
    "            close_pairs = np.where(\n",
    "                (distances > 0) & (distances <= self._config.min_distance)\n",
    "            )\n",
    "\n",
    "            peaks_to_keep = set(range(len(peak_coords)))\n",
    "\n",
    "            for i, j in zip(close_pairs[0], close_pairs[1]):\n",
    "                if i < j and i in peaks_to_keep and j in peaks_to_keep:\n",
    "                    if peak_values[i] >= peak_values[j]:\n",
    "                        peaks_to_keep.remove(j)\n",
    "                    else:\n",
    "                        peaks_to_keep.remove(i)\n",
    "\n",
    "            peaks_to_keep = sorted(peaks_to_keep)\n",
    "            merged_rows = adjusted_rows[peaks_to_keep]\n",
    "            merged_cols = adjusted_cols[peaks_to_keep]\n",
    "\n",
    "            return np.stack((merged_rows, merged_cols), axis=1)\n",
    "        else:\n",
    "            return np.empty((0, 2), dtype=int)\n",
    "\n",
    "print(\"âœ“ Peak detector service defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Service - Track Slicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackSlicerService:\n",
    "    \"\"\"Service for slicing tracks based on cotangent ranges.\"\"\"\n",
    "\n",
    "    def __init__(self, easing_strategy: EasingStrategy,\n",
    "                 hough_config: HoughConfig,\n",
    "                 processing_config: ProcessingConfig):\n",
    "        self._easing = easing_strategy\n",
    "        self._hough_config = hough_config\n",
    "        self._processing_config = processing_config\n",
    "\n",
    "    def get_slice_bounds(self, slice_num: int) -> Tuple[Optional[float], Optional[float]]:\n",
    "        if slice_num == -1:\n",
    "            return None, None\n",
    "        else:\n",
    "            lo_cot = self._easing.ease(-32.0 + 2.0 * max(slice_num, 0))\n",
    "            hi_cot = self._easing.ease(-32.0 + 2.0 * min(slice_num + 1, 32))\n",
    "            return lo_cot, hi_cot\n",
    "\n",
    "    def filter_tracks_for_slice(self, tracks: TrackCollection,\n",
    "                                 slice_num: int) -> TrackCollection:\n",
    "        vz_min, vz_max = self._processing_config.vz_range\n",
    "\n",
    "        if slice_num > -1:\n",
    "            lo_cot, _ = self.get_slice_bounds(0)\n",
    "            _, hi_cot = self.get_slice_bounds(32)\n",
    "\n",
    "            filtered = tracks.filter_by_vz_range(vz_min, vz_max)\n",
    "            filtered = filtered.filter_by_cot_range(lo_cot, hi_cot)\n",
    "        else:\n",
    "            lo_cot, _ = self.get_slice_bounds(10)\n",
    "            _, hi_cot = self.get_slice_bounds(22)\n",
    "\n",
    "            filtered = tracks.filter_by_vz_range(vz_min, vz_max)\n",
    "            filtered = filtered.filter_by_cot_range(lo_cot, hi_cot)\n",
    "\n",
    "            size = self._hough_config.square_size\n",
    "            nbin_qpt = self._hough_config.nbin_qpt\n",
    "\n",
    "            final_tracks = []\n",
    "            for track in filtered:\n",
    "                if (track.curv_bin > 0 + size and\n",
    "                    track.curv_bin < nbin_qpt - size):\n",
    "                    final_tracks.append(track)\n",
    "\n",
    "            filtered = TrackCollection(final_tracks)\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, easing_type: str, hough_config: HoughConfig,\n",
    "              processing_config: ProcessingConfig) -> 'TrackSlicerService':\n",
    "        strategy = EasingStrategyFactory.create(easing_type)\n",
    "        return cls(strategy, hough_config, processing_config)\n",
    "\n",
    "print(\"âœ“ Track slicer service defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Service - Hough Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoughMatcherService:\n",
    "    \"\"\"Service for matching peaks with true tracks and extracting squares.\"\"\"\n",
    "\n",
    "    def __init__(self, config: HoughConfig):\n",
    "        self._config = config\n",
    "\n",
    "    def match_and_extract_squares(\n",
    "        self,\n",
    "        accumulator_values: np.ndarray,\n",
    "        reconstructed_peaks: PeakCollection,\n",
    "        true_tracks: TrackCollection\n",
    "    ) -> Tuple[HoughSquareCollection, np.ndarray]:\n",
    "        square_collection = HoughSquareCollection()\n",
    "        assignment_mask = np.zeros(len(true_tracks), dtype=int)\n",
    "        true_tracks_list = list(true_tracks)\n",
    "\n",
    "        for peak in reconstructed_peaks:\n",
    "            square, has_match, matched_idx = self._extract_and_classify_square(\n",
    "                accumulator_values, peak, true_tracks_list, assignment_mask\n",
    "            )\n",
    "\n",
    "            if square is not None:\n",
    "                square_collection.add_square(square)\n",
    "                if has_match and matched_idx is not None:\n",
    "                    assignment_mask[matched_idx] = 1\n",
    "\n",
    "        return square_collection, assignment_mask\n",
    "\n",
    "    def _extract_and_classify_square(\n",
    "        self,\n",
    "        values: np.ndarray,\n",
    "        peak: Peak,\n",
    "        true_tracks: list,\n",
    "        assignment_mask: np.ndarray\n",
    "    ) -> Tuple[HoughSquare, bool, int]:\n",
    "        size = self._config.square_size\n",
    "        tolerance = self._config.tolerance\n",
    "\n",
    "        square_x = peak.x - size\n",
    "        square_y = peak.y - size\n",
    "        center_x = square_x + size\n",
    "        center_y = square_y + size\n",
    "\n",
    "        x_start = max(0, int(square_y))\n",
    "        x_end = min(values.shape[0], int(square_y + 2*size))\n",
    "        y_start = max(0, int(square_x))\n",
    "        y_end = min(values.shape[1], int(square_x + 2*size))\n",
    "\n",
    "        if x_end <= x_start or y_end <= y_start:\n",
    "            return None, False, None\n",
    "\n",
    "        square_region = np.copy(values[x_start:x_end, y_start:y_end])\n",
    "\n",
    "        if square_region.shape != (2*size, 2*size):\n",
    "            return None, False, None\n",
    "\n",
    "        has_match = False\n",
    "        matched_idx = None\n",
    "\n",
    "        for idx, track in enumerate(true_tracks):\n",
    "            if assignment_mask[idx] > 0:\n",
    "                continue\n",
    "\n",
    "            track_peak = Peak(\n",
    "                x=track.curv_bin,\n",
    "                y=track.phi_bin,\n",
    "                height=0\n",
    "            )\n",
    "\n",
    "            if peak.is_within_tolerance(track_peak, tolerance):\n",
    "                has_match = True\n",
    "                matched_idx = idx\n",
    "                break\n",
    "\n",
    "        classification = (\n",
    "            SquareClassification.TRUE_POSITIVE if has_match\n",
    "            else SquareClassification.FALSE_POSITIVE\n",
    "        )\n",
    "\n",
    "        square = HoughSquare(\n",
    "            data=square_region.astype(int),\n",
    "            classification=classification,\n",
    "            center_x=center_x,\n",
    "            center_y=center_y\n",
    "        )\n",
    "\n",
    "        return square, has_match, matched_idx\n",
    "\n",
    "    def update_track_reconstruction_status(\n",
    "        self,\n",
    "        tracks: TrackCollection,\n",
    "        assignment_mask: np.ndarray\n",
    "    ) -> int:\n",
    "        newly_reconstructed = 0\n",
    "        for idx, track in enumerate(tracks):\n",
    "            if assignment_mask[idx] > 0 and not track.is_reconstructed:\n",
    "                track.mark_reconstructed()\n",
    "                newly_reconstructed += 1\n",
    "        return newly_reconstructed\n",
    "\n",
    "print(\"âœ“ Hough matcher service defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Service - Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizerService:\n",
    "    \"\"\"Service for visualizing Hough accumulator with peaks.\"\"\"\n",
    "\n",
    "    def __init__(self, hough_config: HoughConfig, vis_config: VisualizationConfig):\n",
    "        self._hough_config = hough_config\n",
    "        self._vis_config = vis_config\n",
    "        self._plot_count = 0\n",
    "\n",
    "    def draw_hough_accumulator(\n",
    "        self,\n",
    "        values: np.ndarray,\n",
    "        reconstructed_peaks: PeakCollection,\n",
    "        true_tracks: TrackCollection,\n",
    "        slice_num: int\n",
    "    ) -> None:\n",
    "        if not self._vis_config.enabled:\n",
    "            return\n",
    "\n",
    "        if self._plot_count >= self._vis_config.max_plots:\n",
    "            return\n",
    "\n",
    "        start_phi = self._vis_config.start_phi\n",
    "        end_phi = self._vis_config.end_phi\n",
    "        size_reco = self._hough_config.square_size\n",
    "        size_true = self._vis_config.size_true\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 24))\n",
    "        im = ax.imshow(\n",
    "            values[start_phi:end_phi, :],\n",
    "            cmap='viridis',\n",
    "            interpolation='nearest'\n",
    "        )\n",
    "\n",
    "        ax.set_aspect('auto')\n",
    "        plt.colorbar(im, label=\"Counts\")\n",
    "        ax.set_xlabel(\"q/pT\")\n",
    "        ax.set_ylabel(\"phi\")\n",
    "        ax.set_title(f\"Hough Accumulator - Slice {slice_num}\")\n",
    "\n",
    "        for peak in reconstructed_peaks:\n",
    "            square_x = peak.x - size_reco\n",
    "            square_y = peak.y - start_phi - size_reco\n",
    "\n",
    "            rect = patches.Rectangle(\n",
    "                (square_x, square_y),\n",
    "                2*size_reco, 2*size_reco,\n",
    "                linewidth=3,\n",
    "                edgecolor='red',\n",
    "                facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        ymax, ymin = ax.get_ylim()\n",
    "\n",
    "        for track in true_tracks:\n",
    "            square_x = int(track.curv_bin) - size_true\n",
    "            square_y = int(track.phi_bin) - start_phi - size_true\n",
    "\n",
    "            rect = patches.Rectangle(\n",
    "                (square_x, square_y),\n",
    "                2*size_true, 2*size_true,\n",
    "                linewidth=3,\n",
    "                edgecolor=\"yellow\",\n",
    "                facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            if xmin < square_x < xmax and ymin < square_y < ymax:\n",
    "                ax.text(\n",
    "                    square_x, square_y,\n",
    "                    f\"{track.number_of_hits} {round(track.pz_over_pt, 1)}\",\n",
    "                    fontsize=10,\n",
    "                    color='white',\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "\n",
    "        plt.show()\n",
    "        self._plot_count += 1\n",
    "\n",
    "        del values\n",
    "        gc.collect()\n",
    "\n",
    "print(\"âœ“ Visualizer service defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Utils - Progress Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AnalysisStatistics:\n",
    "    \"\"\"Value object for analysis statistics.\"\"\"\n",
    "    total_files: int = 0\n",
    "    processed_files: int = 0\n",
    "    total_events: int = 0\n",
    "    processed_histograms: int = 0\n",
    "    true_tracks_total: int = 0\n",
    "    true_squares: int = 0\n",
    "    false_squares: int = 0\n",
    "    start_time: datetime = None\n",
    "    event_ids: List[int] = field(default_factory=list)\n",
    "\n",
    "    def add_event(self, event_id: int) -> None:\n",
    "        if event_id not in self.event_ids:\n",
    "            self.event_ids.append(event_id)\n",
    "            self.total_events += 1\n",
    "\n",
    "    def add_true_tracks(self, count: int) -> None:\n",
    "        self.true_tracks_total += count\n",
    "\n",
    "    def add_squares(self, true_count: int, false_count: int) -> None:\n",
    "        self.true_squares += true_count\n",
    "        self.false_squares += false_count\n",
    "\n",
    "    def increment_files(self) -> None:\n",
    "        self.processed_files += 1\n",
    "\n",
    "    def increment_histograms(self) -> None:\n",
    "        self.processed_histograms += 1\n",
    "\n",
    "    @property\n",
    "    def total_squares(self) -> int:\n",
    "        return self.true_squares + self.false_squares\n",
    "\n",
    "    @property\n",
    "    def reconstruction_efficiency(self) -> float:\n",
    "        if self.true_tracks_total == 0:\n",
    "            return 0.0\n",
    "        return self.true_squares / self.true_tracks_total\n",
    "\n",
    "\n",
    "class ProgressObserver(ABC):\n",
    "    \"\"\"Abstract base class for progress observers.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_file_start(self, filename: str, file_num: int, total_files: int) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_file_complete(self, filename: str, stats: AnalysisStatistics) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_event_discovered(self, event_id: int) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_slice_start(self, event_id: int, slice_num: int) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_slice_complete(\n",
    "        self,\n",
    "        event_id: int,\n",
    "        slice_num: int,\n",
    "        peaks_found: int,\n",
    "        true_tracks_in_slice: int,\n",
    "        matched_tracks: int\n",
    "    ) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_analysis_start(self, total_files: int) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_analysis_complete(self, stats: AnalysisStatistics) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ConsoleProgressObserver(ProgressObserver):\n",
    "    \"\"\"Console-based progress observer.\"\"\"\n",
    "\n",
    "    def __init__(self, verbose: bool = True):\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def on_file_start(self, filename: str, file_num: int, total_files: int) -> None:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"File {file_num}/{total_files}: {filename}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "    def on_file_complete(self, filename: str, stats: AnalysisStatistics) -> None:\n",
    "        if self._verbose:\n",
    "            print(f\"\\nâœ“ Completed {filename}\")\n",
    "\n",
    "    def on_event_discovered(self, event_id: int) -> None:\n",
    "        print(f\"\\nEvent: {event_id}\")\n",
    "\n",
    "    def on_slice_start(self, event_id: int, slice_num: int) -> None:\n",
    "        if self._verbose:\n",
    "            print(f\"\\n####### Slice number: {slice_num}\")\n",
    "\n",
    "    def on_slice_complete(\n",
    "        self,\n",
    "        event_id: int,\n",
    "        slice_num: int,\n",
    "        peaks_found: int,\n",
    "        true_tracks_in_slice: int,\n",
    "        matched_tracks: int\n",
    "    ) -> None:\n",
    "        print(f\"Peaks found: {peaks_found}\")\n",
    "        print(f\"True tracks in slice: {true_tracks_in_slice}\")\n",
    "        print(f\"Matched tracks: {matched_tracks}\")\n",
    "\n",
    "    def on_analysis_start(self, total_files: int) -> None:\n",
    "        print(\"=\"*80)\n",
    "        print(\"Hough Transform Analysis - Starting Processing\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nTotal files to process: {total_files}\")\n",
    "\n",
    "    def on_analysis_complete(self, stats: AnalysisStatistics) -> None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Processing Complete!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  - Files processed: {stats.processed_files}/{stats.total_files}\")\n",
    "        print(f\"  - Events processed: {stats.total_events}\")\n",
    "        print(f\"  - True tracks found: {stats.true_tracks_total}\")\n",
    "        print(f\"  - True squares: {stats.true_squares}\")\n",
    "        print(f\"  - False squares: {stats.false_squares}\")\n",
    "        print(f\"  - Reconstruction efficiency: {stats.reconstruction_efficiency:.2%}\")\n",
    "\n",
    "print(\"âœ“ Progress observers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Facade - Analysis Facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoughAnalysisFacade:\n",
    "    \"\"\"Facade providing simplified API for Hough transform analysis.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: AnalysisConfig,\n",
    "        observer: Optional[ProgressObserver] = None\n",
    "    ):\n",
    "        self._config = config\n",
    "        self._observer = observer or ConsoleProgressObserver(verbose=True)\n",
    "        self._stats = AnalysisStatistics()\n",
    "\n",
    "        # Initialize repositories\n",
    "        self._particle_repo = ParticleRepository(config.paths.data_path_obj)\n",
    "        self._hough_repo = HoughRepository(config.paths.data_path_obj)\n",
    "        self._training_repo = TrainingDataRepository(config.paths.output_dir_obj)\n",
    "\n",
    "        # Initialize services\n",
    "        self._peak_detector = PeakDetectorService(config.peak_detection)\n",
    "        self._track_slicer = TrackSlicerService.create(\n",
    "            config.easing_type,\n",
    "            config.hough,\n",
    "            config.processing\n",
    "        )\n",
    "        self._hough_matcher = HoughMatcherService(config.hough)\n",
    "        self._visualizer = VisualizerService(config.hough, config.visualization)\n",
    "\n",
    "        # Storage for analysis results\n",
    "        self._tracks_dict = {}\n",
    "        self._square_collection = HoughSquareCollection()\n",
    "\n",
    "    def run_complete_analysis(self) -> AnalysisStatistics:\n",
    "        \"\"\"Run complete end-to-end analysis.\"\"\"\n",
    "        self._stats.start_time = datetime.now()\n",
    "\n",
    "        # Step 1: Load particle data\n",
    "        print(\"\\n[1/4] Loading particle simulation data...\")\n",
    "        df = self._particle_repo.load_particles()\n",
    "\n",
    "        # Step 2: Create true tracks dictionary\n",
    "        print(\"\\n[2/4] Creating true tracks dictionary...\")\n",
    "        self._tracks_dict = self._particle_repo.create_tracks_dict(\n",
    "            nbin_phi=self._config.hough.nbin_phi,\n",
    "            nbin_qpt=self._config.hough.nbin_qpt,\n",
    "            min_hits=self._config.processing.min_hits\n",
    "        )\n",
    "        print(f\"Created tracks for {len(self._tracks_dict)} events\")\n",
    "\n",
    "        # Step 3: Process Hough files\n",
    "        print(\"\\n[3/4] Processing Hough accumulator files...\")\n",
    "        self._process_hough_files()\n",
    "\n",
    "        # Step 4: Save results\n",
    "        print(\"\\n[4/4] Saving results...\")\n",
    "        self._save_results()\n",
    "\n",
    "        # Notify completion\n",
    "        self._observer.on_analysis_complete(self._stats)\n",
    "\n",
    "        return self._stats\n",
    "\n",
    "    def _process_hough_files(self) -> None:\n",
    "        files = self._hough_repo.find_files()\n",
    "        self._stats.total_files = min(len(files), self._config.processing.num_files)\n",
    "        self._observer.on_analysis_start(self._stats.total_files)\n",
    "\n",
    "        for file_idx, file_path in enumerate(files):\n",
    "            if file_idx >= self._config.processing.num_files:\n",
    "                break\n",
    "            self._process_single_file(file_path, file_idx + 1)\n",
    "            self._stats.increment_files()\n",
    "\n",
    "    def _process_single_file(self, file_path, file_num: int) -> None:\n",
    "        self._observer.on_file_start(\n",
    "            file_path.name,\n",
    "            file_num,\n",
    "            self._stats.total_files\n",
    "        )\n",
    "\n",
    "        for hough_hist in self._hough_repo.iter_histograms(file_path):\n",
    "            self._process_histogram(hough_hist)\n",
    "            self._stats.increment_histograms()\n",
    "            gc.collect()\n",
    "\n",
    "        self._observer.on_file_complete(file_path.name, self._stats)\n",
    "\n",
    "    def _process_histogram(self, hough_hist) -> None:\n",
    "        event_id = hough_hist.event_id\n",
    "        slice_num = hough_hist.slice_num\n",
    "\n",
    "        if event_id not in self._stats.event_ids:\n",
    "            self._stats.add_event(event_id)\n",
    "            self._observer.on_event_discovered(event_id)\n",
    "\n",
    "        if slice_num not in self._config.processing.slice_list:\n",
    "            return\n",
    "\n",
    "        self._observer.on_slice_start(event_id, slice_num)\n",
    "\n",
    "        if event_id not in self._tracks_dict:\n",
    "            return\n",
    "\n",
    "        true_tracks = self._tracks_dict[event_id]\n",
    "        filtered_tracks = self._track_slicer.filter_tracks_for_slice(\n",
    "            true_tracks,\n",
    "            slice_num\n",
    "        )\n",
    "\n",
    "        self._stats.add_true_tracks(len(filtered_tracks))\n",
    "\n",
    "        peaks = self._peak_detector.find_peaks(hough_hist.histogram)\n",
    "        values, _, _ = hough_hist.histogram.to_numpy()\n",
    "        values = values.T\n",
    "\n",
    "        squares, assignment_mask = self._hough_matcher.match_and_extract_squares(\n",
    "            values,\n",
    "            peaks,\n",
    "            filtered_tracks\n",
    "        )\n",
    "\n",
    "        matched_count = self._hough_matcher.update_track_reconstruction_status(\n",
    "            filtered_tracks,\n",
    "            assignment_mask\n",
    "        )\n",
    "\n",
    "        for square in [s for s in [squares._true_positives, squares._false_positives] for s in s]:\n",
    "            self._square_collection.add_square(square)\n",
    "\n",
    "        self._stats.add_squares(\n",
    "            squares.true_positive_count,\n",
    "            squares.false_positive_count\n",
    "        )\n",
    "\n",
    "        self._visualizer.draw_hough_accumulator(\n",
    "            values,\n",
    "            peaks,\n",
    "            filtered_tracks,\n",
    "            slice_num\n",
    "        )\n",
    "\n",
    "        self._observer.on_slice_complete(\n",
    "            event_id,\n",
    "            slice_num,\n",
    "            len(peaks),\n",
    "            len(filtered_tracks),\n",
    "            matched_count\n",
    "        )\n",
    "\n",
    "        del values\n",
    "        gc.collect()\n",
    "\n",
    "    def _save_results(self) -> None:\n",
    "        print(\"\\nSaving training data...\")\n",
    "        self._training_repo.save_squares(\n",
    "            self._square_collection,\n",
    "            filename=\"images.npz\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nSaving true tracks to ROOT file...\")\n",
    "        self._particle_repo.save_tracks_to_root(\n",
    "            self._tracks_dict,\n",
    "            self._stats.event_ids,\n",
    "            filename=str(self._config.paths.output_dir_obj / \"out_true_tracks.root\"),\n",
    "            treename=\"ntuple\"\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_default_config(cls) -> 'HoughAnalysisFacade':\n",
    "        config = AnalysisConfigBuilder.from_legacy_config()\n",
    "        return cls(config)\n",
    "\n",
    "print(\"âœ“ Analysis facade defined\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check current working directory and set output path\nimport os\n\n# Get current working directory\ncurrent_dir = os.getcwd()\nprint(f\"Current working directory: {current_dir}\")\nprint(f\"Notebook location: {os.path.dirname(os.path.abspath('analyze_ttbar_OOP.ipynb')) if os.path.exists('analyze_ttbar_OOP.ipynb') else 'unknown'}\")\n\n# Set your desired output directory here\n# Option 1: Use current directory\nOUTPUT_DIR = current_dir\n\n# Option 2: Use specific directory (uncomment and modify as needed)\n# OUTPUT_DIR = \"/path/to/your/output/directory\"\n\n# Option 3: Use same directory as notebook (if running from notebook dir)\n# OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else current_dir\n\nprint(f\"\\nOutput files will be saved to: {OUTPUT_DIR}\")\nprint(f\"  - images.npz\")\nprint(f\"  - out_true_tracks.root\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure the analysis\nconfig = (AnalysisConfigBuilder()\n          .with_paths(\n              data_path=\"/eos/user/t/tbold/EFTracking/HoughML/pg_2mu_pu100_insquare\",\n              output_dir=OUTPUT_DIR  # Use the OUTPUT_DIR from previous cell\n          )\n          .with_processing_config(\n              slice_list=[-1],\n              num_files=8\n          )\n          .build())\n\n# Print configuration\nprint(\"Configuration:\")\nprint(f\"  Data path: {config.paths.data_path}\")\nprint(f\"  Output dir: {config.paths.output_dir}\")\nprint(f\"  Absolute output path: {config.paths.output_dir_obj.absolute()}\")\nprint(f\"  Number of files: {config.processing.num_files}\")\nprint(f\"  Slice list: {config.processing.slice_list}\")\nprint(f\"  Easing type: {config.easing_type}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create facade and run analysis\n",
    "facade = HoughAnalysisFacade(config)\n",
    "stats = facade.run_complete_analysis()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ Analysis completed successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. View Results\n",
    "\n",
    "Examine the analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load saved training data\nimport os\nnpz_file = os.path.join(OUTPUT_DIR, 'images.npz')\n\nprint(f\"Loading training data from: {npz_file}\")\nif not os.path.exists(npz_file):\n    print(f\"ERROR: File not found! Looking in: {npz_file}\")\n    print(f\"Files in output directory:\")\n    for f in os.listdir(OUTPUT_DIR):\n        print(f\"  - {f}\")\nelse:\n    data = np.load(npz_file)\n    X = data['X']\n    y = data['y']\n\n    print(f\"\\nTraining data loaded successfully!\")\n    print(f\"  Features (X): {X.shape}\")\n    print(f\"  Labels (y): {y.shape}\")\n    print(f\"  True positives: {np.sum(y == 1)}\")\n    print(f\"  False positives: {np.sum(y == 0)}\")\n\n    # Show example squares\n    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n    fig.suptitle('Sample Training Data (Top: True Positives, Bottom: False Positives)', fontsize=14)\n\n    # Show true positives\n    true_indices = np.where(y == 1)[0][:5]\n    for i, idx in enumerate(true_indices):\n        axes[0, i].imshow(X[idx], cmap='viridis')\n        axes[0, i].set_title(f'True #{idx}')\n        axes[0, i].axis('off')\n\n    # Show false positives\n    false_indices = np.where(y == 0)[0][:5]\n    for i, idx in enumerate(false_indices):\n        axes[1, i].imshow(X[idx], cmap='viridis')\n        axes[1, i].set_title(f'False #{idx}')\n        axes[1, i].axis('off')\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook contains the complete OOP refactored Hough Transform Analysis system.\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… Domain-Driven Design with clear business entities\n",
    "- âœ… SOLID principles throughout\n",
    "- âœ… 8 design patterns (Facade, Repository, Strategy, Builder, Observer, Singleton, Factory, Value Object)\n",
    "- âœ… Fully testable and maintainable\n",
    "- âœ… Production-ready code quality\n",
    "\n",
    "**To use:** Simply run all cells, or modify the configuration in cell 17 before running the analysis.\n",
    "\n",
    "**Output files:**\n",
    "- `images.npz` - Training data (X and y arrays)\n",
    "- `out_true_tracks.root` - ROOT file with true track information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}