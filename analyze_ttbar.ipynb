{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89df17be",
   "metadata": {},
   "source": [
    "Code to read and write down Hough maxima from data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install particle==0.25.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from scipy.ndimage import maximum_filter, label, find_objects\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "import os\n",
    "import gc  #garbage collector\n",
    "from pathlib import Path\n",
    "\n",
    "#from particle import Particle\n",
    "import hepunits.units as units\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55dd18",
   "metadata": {},
   "source": [
    "Define binning of the Hough accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecef3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbin_phi, nbin_qpt   = 7000, 216\n",
    "\n",
    "# Size (half-width) of the square\n",
    "size = 16\n",
    "\n",
    "# Peak finding\n",
    "threshold_abs=5     # absolute threshold\n",
    "#threshold_abs=5     # absolute threshold\n",
    "threshold_rel=0.0  # peaks > 70% of max\n",
    "min_distance=2      # at least 6 bins apart\n",
    "smooth_sigma=0      # smooth before detection\n",
    "\n",
    "tol = 6             # tolerance between reco and true peak\n",
    "\n",
    "#slice_list = list(range(-1, 33))\n",
    "#slice_list = list(range(12, 20))\n",
    "slice_list = [-1]\n",
    "\n",
    "num_files = 8  # number of files to be processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93cbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/eos/user/t/tbold/EFTracking/HoughML/ttbar_pu10_insquare\"   # directory with ttbar data files\n",
    "#path = \"/eos/user/t/tbold/EFTracking/HoughML/ttbar_pu10_insquare_full\" \n",
    "\n",
    "#path = \"/eos/user/t/tbold/EFTracking/HoughML/ttbar_pu100_insquare\"\n",
    "#path = \"/eos/user/t/tbold/EFTracking/HoughML/ttbar_pu100_insquare_full\"\n",
    "\n",
    "path = \"/eos/user/t/tbold/EFTracking/HoughML/pg_2mu_pu100_insquare\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650f02a",
   "metadata": {},
   "source": [
    "Read simulation file with true tracks information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files = sorted(Path(path).glob(\"particles*.root\"))\n",
    "\n",
    "print(f\"Found {len(root_files)} ROOT files in {path}\\n\")\n",
    "\n",
    "# --- LOOP THROUGH FILES AND LIST OBJECTS ---\n",
    "for file in root_files:\n",
    "    print(f\"üìÅ {file.name}\")\n",
    "    tree = uproot.open(file)[\"particles\"]\n",
    "\n",
    "    # Get data as a dictionary of NumPy arrays\n",
    "    arrays = tree.arrays(library=\"np\")\n",
    "\n",
    "# Convert manually into a DataFrame\n",
    "df = pd.DataFrame(arrays)\n",
    "\n",
    "print(\"Column names:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff9691",
   "metadata": {},
   "source": [
    "Get charges of all particles in event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended particle charge dictionary\n",
    "PARTICLE_CHARGES = {\n",
    "    # Gauge bosons\n",
    "    22: 0,       # photon\n",
    "    23: 0,       # Z boson\n",
    "    24: 1,       # W+\n",
    "    -24: -1,     # W-\n",
    "    21: 0,       # gluon\n",
    "    \n",
    "    # Leptons\n",
    "    11: -1,      # e-\n",
    "    -11: 1,      # e+\n",
    "    12: 0,       # ŒΩ_e\n",
    "    -12: 0,      # ŒΩ_e bar\n",
    "    13: -1,      # Œº-\n",
    "    -13: 1,      # Œº+\n",
    "    14: 0,       # ŒΩ_Œº\n",
    "    -14: 0,      # ŒΩ_Œº bar\n",
    "    15: -1,      # œÑ-\n",
    "    -15: 1,      # œÑ+\n",
    "    16: 0,       # ŒΩ_œÑ\n",
    "    -16: 0,      # ŒΩ_œÑ bar\n",
    "    \n",
    "    # Quarks\n",
    "    1: -1/3,     # d\n",
    "    -1: 1/3,     # d bar\n",
    "    2: 2/3,      # u\n",
    "    -2: -2/3,    # u bar\n",
    "    3: -1/3,     # s\n",
    "    -3: 1/3,     # s bar\n",
    "    4: 2/3,      # c\n",
    "    -4: -2/3,    # c bar\n",
    "    5: -1/3,     # b\n",
    "    -5: 1/3,     # b bar\n",
    "    6: 2/3,      # t\n",
    "    -6: -2/3,    # t bar\n",
    "    \n",
    "    # Light mesons\n",
    "    111: 0,      # œÄ‚Å∞\n",
    "    211: 1,      # œÄ+\n",
    "    -211: -1,    # œÄ-\n",
    "    113: 0,      # œÅ‚Å∞\n",
    "    213: 1,      # œÅ+\n",
    "    -213: -1,    # œÅ-\n",
    "    221: 0,      # Œ∑\n",
    "    331: 0,      # Œ∑'\n",
    "    130: 0,      # K_L‚Å∞\n",
    "    310: 0,      # K_S‚Å∞\n",
    "    311: 0,      # K‚Å∞\n",
    "    -311: 0,     # K‚Å∞ bar\n",
    "    321: 1,      # K+\n",
    "    -321: -1,    # K-\n",
    "    \n",
    "    # Charmed mesons\n",
    "    411: 1,      # D+\n",
    "    -411: -1,    # D-\n",
    "    421: 0,      # D‚Å∞\n",
    "    -421: 0,     # D‚Å∞ bar\n",
    "    \n",
    "    # Bottom mesons\n",
    "    511: 0,      # B‚Å∞\n",
    "    -511: 0,     # B‚Å∞ bar\n",
    "    521: 1,      # B+\n",
    "    -521: -1,    # B-\n",
    "    \n",
    "    # Baryons\n",
    "    2212: 1,     # proton\n",
    "    -2212: -1,   # antiproton\n",
    "    2112: 0,     # neutron\n",
    "    -2112: 0,    # antineutron\n",
    "    3122: 0,     # Œõ\n",
    "    -3122: 0,    # Œõ bar\n",
    "    3222: 1,     # Œ£+\n",
    "    -3222: -1,   # Œ£+ bar\n",
    "    3212: 0,     # Œ£‚Å∞\n",
    "    -3212: 0,    # Œ£‚Å∞ bar\n",
    "    3112: -1,    # Œ£-\n",
    "    -3112: 1,    # Œ£- bar\n",
    "    3312: -1,    # Œû‚Åª\n",
    "    -3312: 1,    # Œû‚Åª bar\n",
    "    3322: 0,     # Œû‚Å∞\n",
    "    -3322: 0,    # Œû‚Å∞ bar\n",
    "}\n",
    "\n",
    "def get_charge_from_pdg(pdg_id):\n",
    "    \"\"\"Get electric charge from PDG ID\"\"\"\n",
    "    if pdg_id in PARTICLE_CHARGES:\n",
    "        return PARTICLE_CHARGES[pdg_id]\n",
    "    else:\n",
    "        # Try to handle antiparticles automatically\n",
    "        if pdg_id < 0 and -pdg_id in PARTICLE_CHARGES:\n",
    "            return -PARTICLE_CHARGES[-pdg_id]\n",
    "        return None\n",
    "\n",
    "# Alternative: return 0 for unknown particles instead of None\n",
    "def get_charge_safe(pdg_id):\n",
    "    \"\"\"Get charge, return 0 for unknown particles\"\"\"\n",
    "    charge = get_charge_from_pdg(pdg_id)\n",
    "    return charge if charge is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3070a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_charges(particle_ids):\n",
    "    \"\"\"\n",
    "    Return a list of charges for a list of PDG IDs.\n",
    "    If an ID is not known in the PDG database, returns None for that entry.\n",
    "    \"\"\"\n",
    "    charges = []\n",
    "    for pid in particle_ids:\n",
    "        try:\n",
    "            #p = Particle.from_pdgid(pid)\n",
    "            #charges.append(int(p.charge))  # convert to int (e units)\n",
    "            charges.append(get_charge_from_pdg(pid))\n",
    "        except Exception:\n",
    "            charges.append(None)  # unknown ID\n",
    "    return charges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacf3d0",
   "metadata": {},
   "source": [
    "Get positions of true tracks on the Hough accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def true_tracks(df, event_id):\n",
    "    \n",
    "    tracks = []\n",
    "    \n",
    "    for row in df.itertuples(index=False):\n",
    "        if row.event_id == event_id: \n",
    "            phi = row.phi\n",
    "            phi_bin = (phi+np.pi)*nbin_phi/(2.*np.pi)\n",
    "            charges = get_charges(row.particle_type)\n",
    "            #print(\"charges \",charges)\n",
    "            \n",
    "            \n",
    "            # mask when charges != 0\n",
    "            mask = (np.array(charges) != 0) & (np.array(row.number_of_hits) > 4)\n",
    "            \n",
    "\n",
    "            curv = charges/row.pt\n",
    "            curv_bin = int(nbin_qpt/2.)+curv*int(nbin_qpt/2.)\n",
    "            #print(type(curv_bin))\n",
    "            accu = np.stack((np.array(phi_bin)[mask], np.array(curv_bin)[mask], \n",
    "                             np.array(row.eta)[mask], np.array(row.vz)[mask], np.array(row.number_of_hits)[mask],\n",
    "                             np.array(row.pz/row.pt)[mask], np.array(row.particle_type)[mask]), axis=1)\n",
    "            \n",
    "            #print(accu.shape, len(phi))\n",
    "\n",
    "            return accu \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def true_tracks(df, event_id):\n",
    "    \n",
    "    result_dfs = []\n",
    "    \n",
    "    for row in df.itertuples(index=False):\n",
    "        if row.event_id == event_id: \n",
    "            phi = row.phi\n",
    "            phi_bin = (phi+np.pi)*nbin_phi/(2.*np.pi)\n",
    "            charges = get_charges(row.particle_type)\n",
    "            \n",
    "            # mask when charges != 0\n",
    "            mask = (np.array(charges) != 0) & (np.array(row.number_of_hits) > 4)\n",
    "            \n",
    "            curv = charges/row.pt\n",
    "            curv_bin = int(nbin_qpt/2.)+curv*int(nbin_qpt/2.)\n",
    "            \n",
    "            # Apply mask to ALL arrays that need filtering\n",
    "            filtered_phi_bin = np.array(phi_bin)[mask]\n",
    "            filtered_curv_bin = np.array(curv_bin)[mask]\n",
    "            filtered_eta = np.array(row.eta)[mask]\n",
    "            filtered_vz = np.array(row.vz)[mask]\n",
    "            filtered_number_of_hits = np.array(row.number_of_hits)[mask]\n",
    "            filtered_pz_over_pt = np.array(row.pz/row.pt)[mask]\n",
    "            filtered_particle_type = np.array(row.particle_type)[mask]\n",
    "            \n",
    "            # Also apply mask to other fields that should be filtered\n",
    "            filtered_phi = np.array(row.phi)[mask] if hasattr(row.phi, '__len__') else np.full(np.sum(mask), row.phi)\n",
    "            filtered_pt = np.array(row.pt)[mask] if hasattr(row.pt, '__len__') else np.full(np.sum(mask), row.pt)\n",
    "            filtered_pz = np.array(row.pz)[mask] if hasattr(row.pz, '__len__') else np.full(np.sum(mask), row.pz)\n",
    "            filtered_event_id = np.full(np.sum(mask), row.event_id)\n",
    "            \n",
    "            # Add 'reco' column with all zeros\n",
    "            filtered_reco = np.zeros(np.sum(mask))\n",
    "            \n",
    "            # Create DataFrame with ALL filtered fields\n",
    "            accu_df = pd.DataFrame({\n",
    "                'phi_bin': filtered_phi_bin,\n",
    "                'curv_bin': filtered_curv_bin,\n",
    "                'eta': filtered_eta,\n",
    "                'vz': filtered_vz,\n",
    "                'number_of_hits': filtered_number_of_hits,\n",
    "                'pz_over_pt': filtered_pz_over_pt,\n",
    "                'particle_type': filtered_particle_type,\n",
    "                # Add additional fields with proper masking\n",
    "                'event_id': filtered_event_id,\n",
    "                'phi': filtered_phi,\n",
    "                'pt': filtered_pt,\n",
    "                'pz': filtered_pz,\n",
    "                'reco': filtered_reco,\n",
    "            })\n",
    "            \n",
    "#            # Add any other fields that exist in the original row (with proper masking)\n",
    "#            for field in row._fields:\n",
    "#                if field not in accu_df.columns and field not in ['Index', 'index']:\n",
    "#                    original_value = getattr(row, field)\n",
    "#                    # Apply mask if it's an array, otherwise broadcast\n",
    "#                    if hasattr(original_value, '__len__') and len(original_value) == len(mask):\n",
    "#                        filtered_value = np.array(original_value)[mask]\n",
    "#                    else:\n",
    "#                        filtered_value = np.full(np.sum(mask), original_value)\n",
    "#                    accu_df[field] = filtered_value\n",
    "#            \n",
    "            result_dfs.append(accu_df)\n",
    "    \n",
    "    # Combine all DataFrames if multiple rows match the event_id\n",
    "    if result_dfs:\n",
    "        return pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8464939",
   "metadata": {},
   "source": [
    "Create true track dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Initialize dictionary that automatically creates empty lists for new keys\n",
    "truetracks = defaultdict(list)\n",
    "\n",
    "# Your loop\n",
    "for event_id in df[\"event_id\"]:\n",
    "    #print(\"Event: \",event_id)\n",
    "    truetracks[event_id].append(true_tracks(df, event_id))\n",
    "    #print(truetracks[event_id])\n",
    "\n",
    "# Convert back to regular dict if needed\n",
    "truetracks = dict(truetracks)\n",
    "#print(truetracks)\n",
    "'''\n",
    "\n",
    "# Initialize as regular dictionary\n",
    "truetracks = {}\n",
    "\n",
    "# Your loop - assign directly instead of appending\n",
    "for event_id in df[\"event_id\"]:\n",
    "    #print(\"Event: \",event_id)\n",
    "    truetracks[event_id] = true_tracks(df, event_id)  # Direct assignment, not append\n",
    "    #print(truetracks[event_id])\n",
    "\n",
    "# No need to convert back since it's already a regular dict\n",
    "#print(truetracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25291294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in truetracks.items():\n",
    "#    print(f\"{key}: {type(value)} - {len(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3e190",
   "metadata": {},
   "source": [
    "Vectorized sliding window to find maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorized_2d_sliding_peaks(matrix, min_height=6, window_size=5):\n",
    "    \"\"\"\n",
    "    Fully vectorized sliding window peak detection for 2D arrays\n",
    "    \"\"\"\n",
    "    if window_size % 2 == 0:\n",
    "        window_size += 1  # Ensure odd window size\n",
    "    \n",
    "    half_window = window_size // 2\n",
    "    peaks = []\n",
    "    \n",
    "    # Create 2D sliding window view - this is the key optimization\n",
    "    # Shape: (rows, cols, window_size, window_size)\n",
    "    windows_2d = sliding_window_view(matrix, (window_size, window_size))\n",
    "    \n",
    "    # Center position in each window\n",
    "    center_r, center_c = half_window, half_window\n",
    "    \n",
    "    # Extract center values and neighbors\n",
    "    center_values = windows_2d[:, :, center_r, center_c]\n",
    "    \n",
    "    # Vectorized peak detection conditions\n",
    "    is_peak = np.ones_like(center_values, dtype=bool)\n",
    " \n",
    "\n",
    "\n",
    "    # Check if center is maximum in the window\n",
    "    window_maxima = np.max(windows_2d, axis=(2, 3))\n",
    "    is_peak &= (center_values == window_maxima)\n",
    "    \n",
    "    # Check minimum height\n",
    "    is_peak &= (center_values >= min_height)\n",
    "    \n",
    "    # Get coordinates of peaks\n",
    "    peak_rows, peak_cols = np.where(is_peak)\n",
    "    \n",
    "    # Adjust coordinates (sliding window shifts indices)\n",
    "    adjusted_rows = peak_rows + half_window\n",
    "    adjusted_cols = peak_cols + half_window\n",
    "    \n",
    "    \n",
    "    #return np.stack((adjusted_rows, adjusted_cols),axis=1)\n",
    "    \n",
    "    \n",
    "    # Merge close peaks\n",
    "    if len(adjusted_rows) > 0:\n",
    "        # Create array of peak coordinates and values\n",
    "        peak_coords = np.stack((adjusted_rows, adjusted_cols), axis=1)\n",
    "        peak_values = matrix[adjusted_rows, adjusted_cols]\n",
    "        \n",
    "        # Calculate distance matrix between all peaks\n",
    "        from scipy.spatial.distance import cdist\n",
    "        distances = cdist(peak_coords, peak_coords)\n",
    "        \n",
    "        # Find peaks that are too close to each other\n",
    "        close_pairs = np.where((distances > 0) & (distances <= min_distance))\n",
    "        \n",
    "        # Create a set of peaks to keep (initially all)\n",
    "        peaks_to_keep = set(range(len(peak_coords)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Merge close peaks by keeping the one with highest value\n",
    "        for i, j in zip(close_pairs[0], close_pairs[1]):\n",
    "            if i < j and i in peaks_to_keep and j in peaks_to_keep:\n",
    "                if peak_values[i] >= peak_values[j]:\n",
    "                    peaks_to_keep.remove(j)\n",
    "                else:\n",
    "                    peaks_to_keep.remove(i)\n",
    "        \n",
    "        # Convert back to sorted list of indices\n",
    "        peaks_to_keep = sorted(peaks_to_keep)\n",
    "        \n",
    "        # Get the merged peaks\n",
    "        merged_rows = adjusted_rows[peaks_to_keep]\n",
    "        merged_cols = adjusted_cols[peaks_to_keep]\n",
    "        \n",
    "        return np.stack((merged_rows, merged_cols), axis=1)\n",
    "    else:\n",
    "        return np.empty((0, 2), dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4b9a5",
   "metadata": {},
   "source": [
    "Now we go to the hits and Hough accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_local_maxima_2d(hist_obj, threshold_abs=None, threshold_rel=0.1,\n",
    "                         min_distance=5, smooth_sigma=1.5):\n",
    "    \"\"\"\n",
    "    Find local maxima in a 2D ROOT histogram (TH2F/TH2D).\n",
    "    Returns a list of (x, y) coordinates of peaks.\n",
    "    \"\"\"\n",
    "    values, xedges, yedges = hist_obj.to_numpy()\n",
    "    values = values.T\n",
    "    values = np.nan_to_num(values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # smooth histogram to suppress noise\n",
    "    if smooth_sigma > 0:\n",
    "        values = gaussian_filter(values, sigma=smooth_sigma)\n",
    "\n",
    "    # find peaks (returns array of [y, x] indices)\n",
    "    #coords = peak_local_max(values, threshold_abs=threshold_abs,\n",
    "    #                        threshold_rel=threshold_rel,\n",
    "    #                        min_distance=min_distance)\n",
    "    coords = vectorized_2d_sliding_peaks(values, min_height=threshold_abs, window_size=2*min_distance)\n",
    "    #print(\"coords \", type(coords), coords.shape)\n",
    "\n",
    "    xcenters = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "    ycenters = 0.5 * (yedges[1:] + yedges[:-1])\n",
    "\n",
    "    peaks = []\n",
    "    for (y_idx, x_idx) in coords:\n",
    "        peaks.append((xcenters[x_idx], ycenters[y_idx], values[y_idx, x_idx]))\n",
    "\n",
    "    del xedges, yedges, xcenters, ycenters, coords\n",
    "    gc.collect()\n",
    "\n",
    "    return peaks, values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e874711",
   "metadata": {},
   "source": [
    "Get event and slice number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_slice(text):\n",
    "    # Split by common delimiters and check for exact match\n",
    "    parts = text.replace('_', ';').split(';')\n",
    "    \n",
    "    return int(parts[1]), int(parts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8da6b",
   "metadata": {},
   "source": [
    "Check true slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New easing function\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class HoughMeasurementStruct:\n",
    "    cot: float\n",
    "    vz: float\n",
    "\n",
    "class HoughSlicer:\n",
    "    def __init__(self, easing_type: str = \"InSquare\"):\n",
    "        self.easing_type = easing_type\n",
    "    \n",
    "    def easing(self, x: float) -> float:\n",
    "        \"\"\"Easing function implementation\"\"\"\n",
    "        if self.easing_type == \"InSine\":\n",
    "            sign = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "            return sign * 32 * (1 - math.cos((x * math.pi) / 64))\n",
    "        elif self.easing_type == \"InSquare\":\n",
    "            sign = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "            return sign * 32 * (x * x / 1024)\n",
    "        elif self.easing_type == \"InCubic\":\n",
    "            return 32 * (x * x * x / 32768)\n",
    "        elif self.easing_type == \"InCirc\":\n",
    "            sign = 1 if x > 0 else (-1 if x < 0 else 0)\n",
    "            return sign * (32 - math.sqrt(1024 - x * x))\n",
    "        else:  # Linear\n",
    "            return x\n",
    "    \n",
    "    def __call__(self, slice: int) -> (float, float):\n",
    "        \"\"\"Main slicer function\"\"\"\n",
    "        if slice == -1:\n",
    "            return -200 < meas.vz < 200\n",
    "        else:\n",
    "            lo_cot = self.easing(-32.0 + 2.0 * max(slice , 0) )\n",
    "            hi_cot = self.easing(-32.0 + 2.0 * min(slice + 1 , 32) )  # we take true tracks from 1 neighbouring slice !!!!!\n",
    "            #v1 = (meas.vz + 200) / meas.eta\n",
    "            #v2 = (meas.vz - 200) / meas.eta\n",
    "            #print(lo_cot, hi_cot)\n",
    "\n",
    "            #return (meas.cot - lo_cot) * (meas.cot - hi_cot) < 0 and -200 < meas.vz < 200\n",
    "            return lo_cot, hi_cot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accca6fc",
   "metadata": {},
   "source": [
    "Draw hough accumulator and true/reco peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_Hough(values, peaks, slice, true_peaks):\n",
    "    \n",
    "        #values = values0.copy()\n",
    "        \n",
    "        start_phi = 1000\n",
    "        end_phi   = 2000\n",
    "        size_true = 3\n",
    "\n",
    "        \n",
    "        # 2Create a figure and axes, then display the image\n",
    "        fig, ax = plt.subplots(figsize=(16, 2*12))\n",
    "        im = ax.imshow(values[start_phi:end_phi,:], cmap='viridis', interpolation='nearest')  \n",
    "        # Adjust the aspect ratio to allow non-square cells\n",
    "        ax = plt.gca()  # Get the current axes\n",
    "        ax.set_aspect('auto')  # Allow non-square pixels\n",
    "\n",
    "        plt.colorbar(im, label=\"Counts\")  # Add color bar for values\n",
    "        ax.set_xlabel(\"q/pT\")  # Modify as per your data\n",
    "        ax.set_ylabel(\"phi\")  # Modify as per your data\n",
    "        ax.set_title(\"Hough accumulator\")  # Add a title\n",
    "\n",
    "        # Patch a square around reco peaks\n",
    "        for peak in peaks:\n",
    "            # Define the square's properties\n",
    "            square_x = peak[0] - size  # Bottom-left x-coordinate\n",
    "            square_y = peak[1] - start_phi - size  # Bottom-left y-coordinate\n",
    "            #print(square_x, square_y)\n",
    "\n",
    "            # Create a Rectangle patch\n",
    "            # The 'linewidth' defines the border thickness, 'edgecolor' the border color.\n",
    "            # Use 'facecolor=\"none\"' for a hollow square or remove it for a filled one.\n",
    "            square = patches.Rectangle((square_x, square_y), 2*size, 2*size,\n",
    "                                     linewidth=3, edgecolor='red', facecolor='none')\n",
    "\n",
    "            # Add the square to the axes\n",
    "            ax.add_patch(square)\n",
    "            \n",
    "        # Patch a small square around true peaks\n",
    "        print(true_peaks.shape)\n",
    "        \n",
    "        # Get axis limits (image boundaries)\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        ymax, ymin = ax.get_ylim()\n",
    "        for i in range(true_peaks.shape[0]): \n",
    "            # Define the square's properties\n",
    "            square_x = int(true_peaks[i,1]) - size_true  # Bottom-left x-coordinate\n",
    "            square_y = int(true_peaks[i,0]) - start_phi - size_true  # Bottom-left y-coordinate\n",
    "            #print(square_x, square_y)\n",
    "\n",
    "            # Create a Rectangle patch\n",
    "            # The 'linewidth' defines the border thickness, 'edgecolor' the border color.\n",
    "            # Use 'facecolor=\"none\"' for a hollow square or remove it for a filled one.\n",
    "            square = patches.Rectangle((square_x, square_y), 2*size_true, 2*size_true,\n",
    "                                     linewidth=3, edgecolor=\"yellow\", facecolor='none')\n",
    "\n",
    "            \n",
    "            ax.add_patch(square)\n",
    "            \n",
    "            # Draw number of hits directly on ax\n",
    "            if xmin < square_x < xmax and ymin < square_y < ymax:\n",
    "                ax.text(square_x, square_y, str(int(true_peaks[i, 6]))+\" \"+str(round(true_peaks[i,5],1)),\n",
    "                        fontsize=10, color='white', fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        del values\n",
    "        gc.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa45c4",
   "metadata": {},
   "source": [
    "Save small squres (true and false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940876b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Hough_squares(values, peaks, slice, true_peaks_with_mask, true_squares, false_squares):\n",
    "    \n",
    "    # true_peaks_with_mask is a tuple (true_peaks, mask)\n",
    "    true_peaks, mask = true_peaks_with_mask\n",
    "    \n",
    "    # Process reconstructed peaks and extract squares\n",
    "    # mas of the true peaks assigned to the reconstructed peaks\n",
    "    true_peaks_mask = np.zeros(len(true_peaks), dtype=int)\n",
    "    for peak in peaks:\n",
    "        square_x = peak[0] - size\n",
    "        square_y = peak[1] - size\n",
    "        \n",
    "        # Calculate center of the square\n",
    "        center_x = square_x + size\n",
    "        center_y = square_y + size\n",
    "        \n",
    "        # Create and add rectangle patch\n",
    "        #square = patches.Rectangle((square_x, square_y), 2*size, 2*size,\n",
    "        #                         linewidth=3, edgecolor='red', facecolor='none')\n",
    "        #ax.add_patch(square)\n",
    "        \n",
    "        # Extract the square region from the values array\n",
    "        x_start = max(0, int(square_y))\n",
    "        x_end = min(values.shape[0], int(square_y + 2*size))\n",
    "        y_start = max(0, int(square_x))\n",
    "        y_end = min(values.shape[1], int(square_x + 2*size))\n",
    "        \n",
    "        # Copy the region to a new array\n",
    "        if x_end > x_start and y_end > y_start:\n",
    "            square_region = np.copy(values[x_start:x_end, y_start:y_end])\n",
    "            \n",
    "            # Check if there's a true peak within tolerance\n",
    "            has_true_peak = False\n",
    "            for i in range(true_peaks.shape[0]):\n",
    "                true_x = true_peaks[i, 1]  # x coordinate of true peak\n",
    "                true_y = true_peaks[i, 0]  # y coordinate \n",
    "                \n",
    "                # Calculate distance from center of square to true peak\n",
    "                distance = np.sqrt((center_x - true_x)**2 + (center_y - true_y)**2)\n",
    "                # here the second condition means we want to have one to one assignment between reconstructed\n",
    "                # peaks and true tracks \n",
    "                if distance <= tol and true_peaks_mask[i] < 0.5:\n",
    "                    has_true_peak = True\n",
    "                    true_peaks_mask[i] = 1\n",
    "                    #break\n",
    "            \n",
    "            # Classify as true or false square\n",
    "            if square_region.shape == (2*size, 2*size):\n",
    "                if has_true_peak:\n",
    "                    true_squares.append(square_region.astype(int))\n",
    "                    # Change border color to green for true squares\n",
    "                    #square.set_edgecolor('green')\n",
    "                    #square.set_linewidth(4)\n",
    "                else:\n",
    "                    false_squares.append(square_region.astype(int))\n",
    "                    # Keep red border for false squares\n",
    "    \n",
    "    # Get indices in the original true_peaks array\n",
    "    mask_indices = np.where(mask)[0]\n",
    "    \n",
    "    # Map back to original array indices\n",
    "    result_indices = mask_indices[true_peaks_mask.astype(bool)]\n",
    "    \n",
    "    # create an array of 0 and 1 \n",
    "    result_mask = np.zeros(len(mask))\n",
    "    result_mask[result_indices.astype(int)] = 1\n",
    "    \n",
    "    #print(\"mask_indices \", mask_indices, len(mask_indices))\n",
    "    #print(\"true_peaks_mask\", true_peaks_mask, len(true_peaks_mask))\n",
    "    #print(\"result_indices\", result_indices, len(result_indices))\n",
    "    #print(result_mask)\n",
    "    #print(\"result_mask = mask[true_peaks_mask] result_indices \",len(result_mask), len(mask), len(true_peaks_mask), len(result_indices))\n",
    "\n",
    "    return true_squares, false_squares, result_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114952e",
   "metadata": {},
   "source": [
    "Match true and found peaks, write down small images around peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_write(values, peaks, event, slice, truetracks, true_squares, false_squares, draw=True):\n",
    "    \n",
    "    true_peaks = np.squeeze(np.array(truetracks[event]))\n",
    "    # Create slicer and check, whether the true track fits to the slice\n",
    "    #print(true_peaks.shape)\n",
    "    \n",
    "    slicer = HoughSlicer(easing_type=\"InSquare\")\n",
    "    if slice>-1: # slice 0-32\n",
    "        lo_cot, _ = slicer(0)\n",
    "        _, hi_cot = slicer(32)\n",
    "        #lo_cot, hi_cot = slicer(slice)\n",
    "        mask = ( abs(true_peaks[:,3])<200 ) & ( lo_cot < true_peaks[:,5] ) & ( true_peaks[:,5] < hi_cot )\n",
    "        #print(\"lo_cot, true_peaks[:,5], hi_cot \",lo_cot, true_peaks[:,5], hi_cot)\n",
    "        #print(mask)\n",
    "    else: # slice = -1\n",
    "        lo_cot, _ = slicer(10)\n",
    "        _, hi_cot = slicer(22)\n",
    "        mask = (abs(true_peaks[:,3])<200 ) & ( lo_cot < true_peaks[:,5] ) & ( true_peaks[:,5] < hi_cot ) \\\n",
    "                & (true_peaks[:,1] > 0 + size) & (true_peaks[:,1] < nbin_qpt - size)\n",
    "    \n",
    "    #print(true_peaks[mask].shape)\n",
    "    #print(\"true_peaks: \",true_peaks[mask])\n",
    "    #print(\"reco peaks: \",peaks)\n",
    "    \n",
    "    # increase the n_truetracks\n",
    "    global n_truetracks\n",
    "    n_truetracks = n_truetracks + len(true_peaks[mask])\n",
    "    \n",
    "    \n",
    "    if draw:\n",
    "        draw_Hough(values, peaks, slice, true_peaks[mask])\n",
    "    \n",
    "    #get squares around peaks and indices of the associated true tracks\n",
    "    true_squares, false_squares, result_mask = get_Hough_squares(values, peaks, slice, (true_peaks[mask], mask), \\\n",
    "                                                                    true_squares, false_squares)\n",
    "    \n",
    "    print(\"True tracks in event: \",len(true_peaks[mask]),\" True peaks total: \", len(true_squares))\n",
    "    print(\"number of ones in result_mask \", np.count_nonzero(result_mask))\n",
    "    \n",
    "    # fill the \"reco\" column in truetracks marking the reconstructed true tracks (logical sum)\n",
    "    if len(result_mask) == len(truetracks[event]):\n",
    "        truetracks[event]['reco'] = result_mask.astype(int) | truetracks[event]['reco'].astype(int)\n",
    "    else:\n",
    "        print(\"Error! Length of truetracks[event]['reco'] and result_mask differ: \",len(truetracks[event]['reco']), len(result_mask))\n",
    "    print(\"number of ones in reco \", np.count_nonzero(truetracks[event]['reco']))\n",
    "    \n",
    "    return true_squares, false_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_root_file(file_path, truetracks, true_squares, false_squares, threshold_fraction=0.1, neighborhood_size=3):\n",
    "    \"\"\"\n",
    "    Processes a single ROOT file, finds peaks in all 2D histograms,\n",
    "    returns dict {hist_name: [(x, y), ...]}.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Processing file: \",file_path)\n",
    "          \n",
    "    results = {}\n",
    "    ikey = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    with uproot.open(file_path) as f:\n",
    "        for key in f.keys():  # iterate only over keys\n",
    "            #if ikey>67:\n",
    "            #    continue\n",
    "            ikey +=1\n",
    "            \n",
    "            obj = f[key]      # load only one object at a time\n",
    "\n",
    "            if not obj.classname.startswith(\"TH2\"):\n",
    "                continue\n",
    "            #print(obj.name)  \n",
    "                        \n",
    "            #find true maxima\n",
    "            event, slice = event_slice(key)\n",
    "            \n",
    "            #add event into the event list\n",
    "            global event_list\n",
    "            if event not in event_list:\n",
    "                event_list.append(event)\n",
    "                print(\"Event: \", event)\n",
    "                \n",
    "            \n",
    "            #match true and found peaks\n",
    "            if slice in slice_list:\n",
    "                \n",
    "                peaks, values = find_local_maxima_2d(obj,\n",
    "                                     threshold_abs=threshold_abs,  # absolute threshold\n",
    "                                     threshold_rel=threshold_rel,  # peaks > % of max\n",
    "                                     min_distance=min_distance,    # at least n bins apart\n",
    "                                     smooth_sigma=smooth_sigma)    # smooth before detection\n",
    "                \n",
    "                print(\"####### Slice number: \", slice)\n",
    "                print(\"Peaks: \",len(peaks))\n",
    "                true_squares, false_squares = match_write(values, peaks, event, slice, truetracks, \n",
    "                                                          true_squares, false_squares, ikey<200)\n",
    "\n",
    "                del values\n",
    "                \n",
    "            # cleanup per histogram\n",
    "            del obj\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    \n",
    "    gc.collect()\n",
    "    return true_squares, false_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b340d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIND ROOT FILES ---\n",
    "root_files = sorted(Path(path).glob(\"out*.root\"))\n",
    "\n",
    "print(f\"Found {len(root_files)} ROOT files in {path}\\n\")\n",
    "\n",
    "# squares to be saved for NN training\n",
    "true_squares = []\n",
    "false_squares = []\n",
    "\n",
    "# no of true tracks after selection\n",
    "n_truetracks = 0\n",
    "\n",
    "# list of events processed\n",
    "event_list = []\n",
    "\n",
    "# Create empty DataFrame\n",
    "#mc_tracks  = pd.DataFrame(columns=['phi_bin', 'curv_bin', \n",
    "#                             'eta', 'vz', 'number_of_hits',\n",
    "#                             'pz_over_pt', 'particle_type', 'reco'])\n",
    "\n",
    "\n",
    "# --- LOOP THROUGH FILES AND LIST OBJECTS ---\n",
    "ifile=0\n",
    "for file in root_files:\n",
    "    print(\"ifile = \",ifile)\n",
    "    if ifile<num_files:\n",
    "        true_squares, false_squares = process_root_file(file, truetracks, true_squares, false_squares)\n",
    "\n",
    "    ifile += 1\n",
    "    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of arrays into a single numpy array of shape (1000, 28, 28)\n",
    "array_true = np.stack(true_squares, axis=0)  # Stack along a new dimension\n",
    "array_false = np.stack(false_squares, axis=0)  # Stack along a new dimension\n",
    "\n",
    "#add column with class\n",
    "y_true = np.ones(array_true.shape[0])\n",
    "y_false = np.zeros(array_false.shape[0])\n",
    "print(\"True and fake peaks: \",array_true.shape[0], \", \",array_false.shape[0])\n",
    "\n",
    "print(\"Number of true tracks: \", n_truetracks)\n",
    "\n",
    "# Combine arrays along the 0 axis\n",
    "combined_array = np.vstack((array_true, array_false))\n",
    "y_combined = np.hstack((y_true, y_false))\n",
    "\n",
    "# Save the array to disk\n",
    "np.savez('images', X = combined_array, y = y_combined)\n",
    "\n",
    "print(\"Shapes of output arrays: \",combined_array.shape, y_combined.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758d5be",
   "metadata": {},
   "source": [
    "Convert dictionary of DataFrames to single DataFrame and save as ROOT ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59947e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_root_ntuple(df_dict, event_list, filename, treename=\"ntuple\", add_source_id=True):\n",
    "    \"\"\"\n",
    "    Convert dictionary of DataFrames to single DataFrame and save as ROOT ntuple\n",
    "\n",
    "    Parameters:\n",
    "    df_dict: dictionary of {key: DataFrame}\n",
    "    filename: output ROOT file name\n",
    "    treename: name of the TTree\n",
    "    add_source_id: whether to add a column identifying the source DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    combined_dfs = []\n",
    "\n",
    "    for key, df in df_dict.items():\n",
    "        if key in event_list:\n",
    "            df_copy = df.copy()\n",
    "            #print(key, type(df_copy))\n",
    "            # Add identifier column to track original source\n",
    "            if add_source_id:\n",
    "                df_copy['source_df'] = key\n",
    "\n",
    "            combined_dfs.append(df_copy)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "\n",
    "    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "    print(f\"Columns: {combined_df.columns.tolist()}\")\n",
    "\n",
    "    # Prepare data for uproot: convert DataFrame columns to dictionary of NumPy arrays\n",
    "    # Infer data types for mktree\n",
    "    inferred_types = {col: combined_df[col].dtype for col in combined_df.columns}\n",
    "    data_for_uproot = {col: combined_df[col].values for col in combined_df.columns}\n",
    "\n",
    "    '''\n",
    "    # Save as ROOT ntuple using uproot\n",
    "    with uproot.recreate(filename) as file:\n",
    "        file.mktree(treename, data_for_uproot, title=treename)\n",
    "\n",
    "    print(f\"Successfully saved to {filename}\")\n",
    "    return combined_df\n",
    "    '''\n",
    "\n",
    "    # Save as ROOT ntuple using uproot - SIMPLER APPROACH\n",
    "    with uproot.recreate(filename) as file:\n",
    "        file[treename] = combined_df.to_dict(orient='list')  # Convert to dict of lists\n",
    "    \n",
    "    print(f\"Successfully saved to {filename}\")\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56335cf8",
   "metadata": {},
   "source": [
    "Write true tracks as a ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e231904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(truetracks))\n",
    "combined_truetracks = dict_to_root_ntuple(truetracks, event_list, \"out_true_tracks.root\", add_source_id=False)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
